{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1b4469",
   "metadata": {},
   "source": [
    "See example: https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4b869e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d307274",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63cb8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('20230902_pa_in-first_pass_ASR - final.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d6d659a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_id</th>\n",
       "      <th>final_transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4158589786049215315.wav</td>\n",
       "      <td>ਫਿਰ ਵੀ ਜਨਤਕ ਆਵਾਜਾਈ ਅਤੇ ਹੋਰ ਸਹੂਲਤਾਂ ਵਿੱਚ ਵੀ ਸਪੈ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9657143325094795831.wav</td>\n",
       "      <td>ਕੈਪਸੂਲ ਬਹੁਤ ਹੱਦ ਤੱਕ ਅਸਮਾਨ ਵਿੱਚ ਇੱਕ ਟੁੱਟਦੇ ਤਾਰੇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5213202234718806513.wav</td>\n",
       "      <td>ਕਠੋਰ ਬਰਫ਼ ਤੋਂ ਇਲਾਵਾ ਮੌਸਮ ਦੇ ਬੱਤਰ ਹਾਲਾਤ ਬਚਾਅ ਦੀਆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15570111678386919739.wav</td>\n",
       "      <td>ਇਹ ਪ੍ਰਮਾਣਿਕ ਭਾਈਚਾਰਾ ਵੈਨਕੂਵਰ ਦੇ ਬਿਲਕੁਲ ਨੇੜੇ ਹੋਵ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91476501351425231.wav</td>\n",
       "      <td>ਰਜਿਸਟਰੇਸ਼ਨ ਵੀਜ਼ਾ ਪ੍ਰਕਿਰਿਆ ਲਈ ਇਕ ਵਾਧੂ ਜ਼ਰੂਰਤ ਹੈ ਕੁ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>2866916614495344409.wav</td>\n",
       "      <td>ਪ੍ਰਕਿਰਤੀਵਾਦੀਆਂ ਅਤੇ ਫ਼ਿਲਾਸਫ਼ਰਾਂ ਨੇ ਕਲਾਸਿਕਲ ਲਿਖਤਾਂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>16374901297831683881.wav</td>\n",
       "      <td>ਸਿਧਾਂਤ ਨੇ ਫਿਰ ਇਸ ਵਿਚਾਰ ਨੂੰ ਜਨਮ ਦਿੱਤਾ ਕਿ ਮਾਓਰੀ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>18212421733958883362.wav</td>\n",
       "      <td>ਇਹ ਦਲੀਲ ਵਜੋਂ ਪੜ੍ਹਨਾ ਵੀ ਆਸਾਨ ਬਣਾ ਦਿੰਦਾ ਹੈ ਹਾਲਾਂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>14488304692540900480.wav</td>\n",
       "      <td>ਅੰਤਰਰਾਸ਼ਟਰੀ ਓਲੰਪਿਕ ਕਮੇਟੀ ਨੇ ਅੱਜ ਬਰਲਿਨ ਵਿੱਚ ਆਪਣੀ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>15561664568587805328.wav</td>\n",
       "      <td>ਦਿਲਚਸਪ ਗੱਲ ਇਹ ਹੈ ਕਿ ਉਸਨੂੰ ਪ੍ਰਾਚੀਨ ਕਾਲ ਵਿੱਚ ਜਿਆ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2746 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       clip_id  \\\n",
       "0      4158589786049215315.wav   \n",
       "1      9657143325094795831.wav   \n",
       "2      5213202234718806513.wav   \n",
       "3     15570111678386919739.wav   \n",
       "4        91476501351425231.wav   \n",
       "...                        ...   \n",
       "2741   2866916614495344409.wav   \n",
       "2742  16374901297831683881.wav   \n",
       "2743  18212421733958883362.wav   \n",
       "2744  14488304692540900480.wav   \n",
       "2745  15561664568587805328.wav   \n",
       "\n",
       "                                    final_transcription  \n",
       "0     ਫਿਰ ਵੀ ਜਨਤਕ ਆਵਾਜਾਈ ਅਤੇ ਹੋਰ ਸਹੂਲਤਾਂ ਵਿੱਚ ਵੀ ਸਪੈ...  \n",
       "1     ਕੈਪਸੂਲ ਬਹੁਤ ਹੱਦ ਤੱਕ ਅਸਮਾਨ ਵਿੱਚ ਇੱਕ ਟੁੱਟਦੇ ਤਾਰੇ...  \n",
       "2     ਕਠੋਰ ਬਰਫ਼ ਤੋਂ ਇਲਾਵਾ ਮੌਸਮ ਦੇ ਬੱਤਰ ਹਾਲਾਤ ਬਚਾਅ ਦੀਆ...  \n",
       "3     ਇਹ ਪ੍ਰਮਾਣਿਕ ਭਾਈਚਾਰਾ ਵੈਨਕੂਵਰ ਦੇ ਬਿਲਕੁਲ ਨੇੜੇ ਹੋਵ...  \n",
       "4     ਰਜਿਸਟਰੇਸ਼ਨ ਵੀਜ਼ਾ ਪ੍ਰਕਿਰਿਆ ਲਈ ਇਕ ਵਾਧੂ ਜ਼ਰੂਰਤ ਹੈ ਕੁ...  \n",
       "...                                                 ...  \n",
       "2741  ਪ੍ਰਕਿਰਤੀਵਾਦੀਆਂ ਅਤੇ ਫ਼ਿਲਾਸਫ਼ਰਾਂ ਨੇ ਕਲਾਸਿਕਲ ਲਿਖਤਾਂ...  \n",
       "2742  ਸਿਧਾਂਤ ਨੇ ਫਿਰ ਇਸ ਵਿਚਾਰ ਨੂੰ ਜਨਮ ਦਿੱਤਾ ਕਿ ਮਾਓਰੀ ...  \n",
       "2743  ਇਹ ਦਲੀਲ ਵਜੋਂ ਪੜ੍ਹਨਾ ਵੀ ਆਸਾਨ ਬਣਾ ਦਿੰਦਾ ਹੈ ਹਾਲਾਂ...  \n",
       "2744  ਅੰਤਰਰਾਸ਼ਟਰੀ ਓਲੰਪਿਕ ਕਮੇਟੀ ਨੇ ਅੱਜ ਬਰਲਿਨ ਵਿੱਚ ਆਪਣੀ...  \n",
       "2745  ਦਿਲਚਸਪ ਗੱਲ ਇਹ ਹੈ ਕਿ ਉਸਨੂੰ ਪ੍ਰਾਚੀਨ ਕਾਲ ਵਿੱਚ ਜਿਆ...  \n",
       "\n",
       "[2746 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd34ff6",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7788595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = data_df[\"final_transcription\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ca6ebe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ਫਿਰ ਵੀ ਜਨਤਕ ਆਵਾਜਾਈ ਅਤੇ ਹੋਰ ਸਹੂਲਤਾਂ ਵਿੱਚ ਵੀ ਸਪੈਨਿਸ਼ ਬਹੁਤ ਜ਼ਿਆਦਾ ਵਰਤੀ ਜਾਂਦੀ ਹੈ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11702927",
   "metadata": {},
   "source": [
    "## Save transcripts in a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea88fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcripts = pd.DataFrame(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "925f9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcripts.to_csv('transcripts_pa.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b4b98",
   "metadata": {},
   "source": [
    "## Sentence Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffdf5346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=transcripts_pa.txt --model_prefix=m_256Vocab --vocab_size=256\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: transcripts_pa.txt\n",
      "  input_format: \n",
      "  model_prefix: m_256Vocab\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 256\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: transcripts_pa.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2747 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=334580\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9874% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=54\n",
      "trainer_interface.cc(559) LOG(INFO) Final char"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acter coverage=0.999874\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2747 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=205595\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 11780 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 2747\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 7445\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 7445 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6389 obj=11.1239 num_tokens=15059 num_tokens/piece=2.35702\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5294 obj=9.91531 num_tokens=15169 num_tokens/piece=2.86532\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3970 obj=9.90109 num_tokens=16114 num_tokens/piece=4.05894\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3964 obj=9.8281 num_tokens=16118 num_tokens/piece=4.06609\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2973 obj=10.0174 num_tokens=17796 num_tokens/piece=5.98587\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2972 obj=9.94635 num_tokens=17800 num_tokens/piece=5.98923\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2229 obj=10.2172 num_tokens=19680 num_tokens/piece=8.82907\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2229 obj=10.1313 num_tokens=19682 num_tokens/piece=8.82997\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1671 obj=10.4708 num_tokens=21735 num_tokens/piece=13.0072\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1671 obj=10.3755 num_tokens=21736 num_tokens/piece=13.0078\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1253 obj=10.7637 num_tokens=23827 num_tokens/piece=19.016\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1253 obj=10.6616 num_tokens=23828 num_tokens/piece=19.0168\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=939 obj=11.1242 num_tokens=25943 num_tokens/piece=27.6283\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=939 obj=11.0112 num_tokens=25943 num_tokens/piece=27.6283\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=704 obj=11.5302 num_tokens=27901 num_tokens/piece=39.6321\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=704 obj=11.4019 num_tokens=27901 num_tokens/piece=39.6321\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=528 obj=11.9213 num_tokens=30058 num_tokens/piece=56.928\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=528 obj=11.7925 num_tokens=30058 num_tokens/piece=56.928\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=396 obj=12.3238 num_tokens=32028 num_tokens/piece=80.8788\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=396 obj=12.1895 num_tokens=32028 num_tokens/piece=80.8788\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=297 obj=12.7595 num_tokens=33730 num_tokens/piece=113.569\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=297 obj=12.6378 num_tokens=33730 num_tokens/piece=113.569\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=281 obj=12.7423 num_tokens=34195 num_tokens/piece=121.69\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=281 obj=12.7159 num_tokens=34195 num_tokens/piece=121.69\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m_256Vocab.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_256Vocab.vocab\n"
     ]
    }
   ],
   "source": [
    "# train sentencepiece model and makes `m.model` and `m.vocab`\n",
    "spm.SentencePieceTrainer.train('--input=transcripts_pa.txt --model_prefix=m_256Vocab --vocab_size=256')\n",
    "\n",
    "# makes segmenter instance and loads the model file\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m_256Vocab.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a7d3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ਰ', 'ਫ', 'ਿ', 'ਰ', '▁ਵ', 'ੀ', '▁ਜ', 'ਨ', 'ਤ', 'ਕ', '▁ਆ', 'ਵ', 'ਾ', 'ਜ', 'ਾ', 'ਈ', '▁ਅਤੇ', '▁ਹੋ', 'ਰ', '▁ਸ', 'ਹ', 'ੂ', 'ਲ', 'ਤ', 'ਾਂ', '▁ਵਿੱਚ', '▁ਵ', 'ੀ', '▁ਸ', 'ਪ', 'ੈ', 'ਨ', 'ਿ', 'ਸ਼', '▁ਬਹੁਤ', '▁', 'ਜ਼', 'ਿਆ', 'ਦਾ', '▁ਵ', 'ਰ', 'ਤੀ', '▁ਜਾਂ', 'ਦ', 'ੀ', '▁ਹੈ']\n",
      "[101, 7, 4, 36, 5, 39, 8, 13, 11, 71, 22, 3, 45, 3, 31, 48, 59, 4, 9, 23, 44, 10, 13, 16, 34, 36, 5, 9, 42, 33, 8, 7, 40, 119, 12, 65, 63, 68, 36, 4, 86, 84, 32, 5, 28]\n"
     ]
    }
   ],
   "source": [
    "#print(sp.encode_as_pieces('ਰਫਿਰ ਵੀ ਜਨਤਕ ਆਵਾਜਾਈ ਅਤੇ ਹੋਰ ਸਹੂਲਤਾਂ ਵਿੱਚ ਵੀ ਸਪੈਨਿਸ਼ ਬਹੁਤ ਜ਼ਿਆਦਾ ਵਰਤੀ ਜਾਂਦੀ ਹੈ'))\n",
    "#print(sp.encode_as_ids('ਫਿਰ ਵੀ ਜਨਤਕ ਆਵਾਜਾਈ ਅਤੇ ਹੋਰ ਸਹੂਲਤਾਂ ਵਿੱਚ ਵੀ ਸਪੈਨਿਸ਼ ਬਹੁਤ ਜ਼ਿਆਦਾ ਵਰਤੀ ਜਾਂਦੀ ਹੈ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a0430de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ਰ', 'ਫ', 'ਿ', 'ਰ', '▁ਵੀ', '▁ਜ', 'ਨ', 'ਤ', 'ਕ', '▁ਆ', 'ਵਾ', 'ਜ', 'ਾਈ', '▁ਅਤੇ', '▁ਹੋ', 'ਰ', '▁ਸ', 'ਹ', 'ੂ', 'ਲ', 'ਤ', 'ਾਂ', '▁ਵਿੱਚ', '▁ਵੀ', '▁ਸ', 'ਪ', 'ੈ', 'ਨ', 'ਿ', 'ਸ਼', '▁ਬਹੁਤ', '▁ਜ਼ਿਆਦਾ', '▁ਵਰਤ', 'ੀ', '▁ਜਾਂ', 'ਦੀ', '▁ਹੈ']\n",
      "[88, 14, 4, 97, 44, 7, 19, 9, 57, 77, 43, 82, 35, 59, 4, 11, 40, 47, 8, 19, 12, 23, 97, 11, 33, 24, 7, 14, 39, 143, 182, 192, 6, 90, 101, 20]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('ਰਫਿਰ ਵੀ ਜਨਤਕ ਆਵਾਜਾਈ ਅਤੇ ਹੋਰ ਸਹੂਲਤਾਂ ਵਿੱਚ ਵੀ ਸਪੈਨਿਸ਼ ਬਹੁਤ ਜ਼ਿਆਦਾ ਵਰਤੀ ਜਾਂਦੀ ਹੈ'))\n",
    "print(sp.encode_as_ids('ਫਿਰ ਵੀ ਜਨਤਕ ਆਵਾਜਾਈ ਅਤੇ ਹੋਰ ਸਹੂਲਤਾਂ ਵਿੱਚ ਵੀ ਸਪੈਨਿਸ਼ ਬਹੁਤ ਜ਼ਿਆਦਾ ਵਰਤੀ ਜਾਂਦੀ ਹੈ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7cdd9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ੀ ਜਾਂਦੀ ਹੈ\n"
     ]
    }
   ],
   "source": [
    "# decode: id => text\n",
    "print(sp.decode_ids([6, 90, 101, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9b15798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "ਵ\n",
      "123\n",
      "0\n",
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(sp.get_piece_size())\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(sp.id_to_piece(28))\n",
    "print(sp.piece_to_id('ਫ'))\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
    "\n",
    "# , ,  are defined by default. Their ids are (0, 1, 2)\n",
    "#  and  are defined as 'control' symbol.\n",
    "for id in range(3):\n",
    "  print(sp.id_to_piece(id), sp.is_control(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16ca60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "#vocabs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
